# Improving Inference Efficiency
当深度学习（DL）模型部署在边缘设备上时，推理效率通常是
不令人满意的；不满足的；不符合要求的这些问题主要来自于经过训练的网络的大小，因为它需要大量的计算。因此，当在边缘设备上部署DL模型时，许多工程师和科学家经常牺牲准确性来换取速度。此外，由于边缘设备的存储空间通常有限，他们专注于缩小型号大小。

会讲到几个概念
- network quantization
- weight sharing 
- network pruning
- knowledge distillation
- network architecture search

