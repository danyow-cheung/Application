{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "model.train(\n",
    "    x = None,\n",
    "    y = None,\n",
    "    batch_size = None,\n",
    "    epochs=1,\n",
    "    verbose = 'auto',\n",
    "    callbacks= None,\n",
    "    validation_split=0.0,\n",
    "    validation_data =None,shuffle =True,\n",
    "    class_weight=None, sample_weight=None,\n",
    "    initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None,\n",
    "    validation_freq=1, max_queue_size=10, workers=1,\n",
    "    use_multiprocessing=False\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "train_acc_metric=  tf.keras.metrics.CategoricalAccuracy()\n",
    "for epoch in range(epochs):\n",
    "    for step,(x_batch_trian,y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_trian,training=True)\n",
    "            loss_value = loss_fn(y_batch_train,logits)\n",
    "        grads = tape.gradient(loss_value,model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "        train_acc_metric.update_state(y, logits)\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
